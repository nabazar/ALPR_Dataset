{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabazar/ALPR_Dataset/blob/main/DQNforLoadBalancingv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from gym.spaces import Tuple , Box\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XADJtyy4re8",
        "outputId": "c820a249-871c-4348-8eb5-0084a8fba8d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_shape, action_size, learning_rate_max=0.001, learning_rate_decay=0.995, gamma=0.75,\n",
        "                 memory_size=2000, batch_size=32, exploration_max=1.0, exploration_min=0.01, exploration_decay=0.995):\n",
        "        self.state_shape = state_shape\n",
        "\n",
        "        self.state_tensor_shape = (-1,) + state_shape\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate_max = learning_rate_max\n",
        "        self.learning_rate = learning_rate_max\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "        self.gamma = gamma\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=2000)\n",
        "        self.batch_size = batch_size\n",
        "        self.exploration_rate = exploration_max\n",
        "        self.exploration_max = exploration_max\n",
        "        self.exploration_min = exploration_min\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # the actual neural network structure\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.keras.layers.Input(shape=self.state_shape))\n",
        "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=self.state_shape))\n",
        "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(tf.keras.layers.Dropout(0.1))\n",
        "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear', name='action_values', kernel_initializer='he_uniform'))\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.push((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, epsilon=None):\n",
        "        if epsilon == None:\n",
        "            epsilon = self.exploration_rate\n",
        "        if np.random.rand() < epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        return np.argmax(self.target_model.predict(state, verbose=0)[0])\n",
        "\n",
        "    def replay(self, episode=0):\n",
        "\n",
        "        if self.memory.length() < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        experiences, indices, weights = self.memory.sample(self.batch_size)\n",
        "        unpacked_experiences = list(zip(*experiences))\n",
        "        states, actions, rewards, next_states, dones = [list(arr) for arr in unpacked_experiences]\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = tf.convert_to_tensor(states)\n",
        "        states = tf.reshape(states, self.state_tensor_shape)\n",
        "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "        next_states = tf.convert_to_tensor(next_states)\n",
        "        next_states = tf.reshape(next_states, self.state_tensor_shape)\n",
        "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
        "\n",
        "        # Compute Q values and next Q values\n",
        "        target_q_values = self.target_model.predict(next_states, verbose=0)\n",
        "        q_values = self.model.predict(states, verbose=0)\n",
        "\n",
        "        # Compute target values using the Bellman equation\n",
        "        max_target_q_values = np.max(target_q_values, axis=1)\n",
        "        targets = rewards + (1 - dones) * self.gamma * max_target_q_values\n",
        "\n",
        "        # Compute TD errors\n",
        "        batch_indices = np.arange(self.batch_size)\n",
        "        q_values_current_action = q_values[batch_indices, actions]\n",
        "        td_errors = targets - q_values_current_action\n",
        "        self.memory.update_priorities(indices, np.abs(td_errors))\n",
        "\n",
        "        # For learning: Adjust Q values of taken actions to match the computed targets\n",
        "        q_values[batch_indices, actions] = targets\n",
        "\n",
        "        loss = self.model.train_on_batch(states, q_values, sample_weight=weights)\n",
        "\n",
        "        self.exploration_rate = self.exploration_max*self.exploration_decay**episode\n",
        "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
        "        self.learning_rate = self.learning_rate_max*self.learning_rate_decay**episode\n",
        "        tf.keras.backend.set_value(self.model.optimizer.learning_rate, self.learning_rate)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model = tf.keras.models.load_model(name)\n",
        "        self.target_model = tf.keras.models.load_model(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)"
      ],
      "metadata": {
        "id": "oeJ0UTjF3zVr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, epsilon=1e-6, alpha=0.8, beta=0.4, beta_increment=0.001):\n",
        "        self.capacity = capacity\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha   # how much prioritisation is used\n",
        "        self.beta = beta    # for importance sampling weights\n",
        "        self.beta_increment = beta_increment\n",
        "        self.priority_buffer = np.zeros(self.capacity)\n",
        "        self.data = []\n",
        "        self.position = 0\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def push(self, experience):\n",
        "        max_priority = np.max(self.priority_buffer) if self.data else 1.0\n",
        "        if len(self.data) < self.capacity:\n",
        "            self.data.append(experience)\n",
        "        else:\n",
        "            self.data[self.position] = experience\n",
        "        self.priority_buffer[self.position] = max_priority\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        priorities = self.priority_buffer[:len(self.data)]\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.data), batch_size, p=probabilities)\n",
        "        experiences = [self.data[i] for i in indices]\n",
        "\n",
        "        total = len(self.data)\n",
        "        weights = (total * probabilities[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment])\n",
        "\n",
        "        return experiences, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, errors):\n",
        "        for idx, error in zip(indices, errors):\n",
        "            self.priority_buffer[idx] = error + self.epsilon"
      ],
      "metadata": {
        "id": "Za35PI2B31xE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "T6fLEd_l5hqW"
      },
      "outputs": [],
      "source": [
        "def createVM(nVM,ProcessTime,QueueLength,ProcessorNumber):\n",
        "  VMs=[]\n",
        "  for i in range(0,nVM):\n",
        "    VMs.append({'name': 'vm'+str(i), 'L_Q': QueueLength[i], 'ProcessTime':ProcessTime[i],'ProcessorNumber':ProcessorNumber[i],'Occuapted_memory':[],'InputTasks':[],'FinishedTasks':[],'Time_of_the_FinishedTasks':[]})\n",
        "  return VMs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7t2LkzkWYqCC"
      },
      "outputs": [],
      "source": [
        "class LoadBalancing:\n",
        "  def __init__(self,nVM,ProcessTime,QueueLength,ProcessorNumber,lower_bound,upper_bound):\n",
        "    self.action_space = spaces.Discrete(nVM)\n",
        "\n",
        "    self.observation_space = spaces.Box(lower_bound, upper_bound, dtype=np.float32)\n",
        "    self.nVM=nVM\n",
        "    self.ProcessTime=ProcessTime\n",
        "    self.ProcessorNumber=ProcessorNumber\n",
        "    self.QueueLength=QueueLength\n",
        "    self.VMs=createVM(self.nVM,self.ProcessTime,self.QueueLength,self.ProcessorNumber)\n",
        "\n",
        "\n",
        "\n",
        "  def step(self,action):\n",
        "    stp=self.stp\n",
        "\n",
        "    task={'name':'T'+str(stp),'id':stp,'Delay':self.VMs[action]['ProcessTime'] ,'VMid': action,'Timer':self.time}\n",
        "\n",
        "    ProcessorNumber=self.ProcessorNumber\n",
        "\n",
        "    for i in range(0,self.nVM):\n",
        "      if len(self.VMs[i]['Occuapted_memory'])>0:\n",
        "        if self.time%self.VMs[i]['ProcessTime']==0 or self.time%self.VMs[i]['ProcessTime']<1:\n",
        "\n",
        "          if ProcessorNumber[i]<=len(self.VMs[i]['Occuapted_memory']):\n",
        "            a=ProcessorNumber[i]\n",
        "\n",
        "          else:\n",
        "            a=len(self.VMs[i]['Occuapted_memory'])\n",
        "          for ia in range(0, a):\n",
        "            self.VMs[i]['FinishedTasks'].append(self.VMs[i]['Occuapted_memory'][ia])\n",
        "            self.VMs[i]['Time_of_the_FinishedTasks'].append(self.time)\n",
        "          self.VMs[i]['Occuapted_memory'][0:a]=[]\n",
        "\n",
        "          self.VMs[i]['L_Q']=self.VMs[i]['L_Q']+a\n",
        "\n",
        "    if self.VMs[action]['L_Q']>0:\n",
        "      self.VMs[action]['L_Q']=self.VMs[action]['L_Q']-1\n",
        "      self.VMs[action]['Occuapted_memory'].append(task['id'])\n",
        "      self.VMs[action]['InputTasks'].append(task['id'])\n",
        "\n",
        "\n",
        "    next_state=[]\n",
        "    for i in range(0,self.nVM):\n",
        "      next_state.append(self.VMs[i]['L_Q'])\n",
        "\n",
        "    C1=len(self.VMs[action]['InputTasks'])>500\n",
        "    C2=len(self.VMs[action]['InputTasks'])<200\n",
        "\n",
        "    reward=-1*C1-1*C2-1*self.VMs[action]['ProcessTime']/30-20*len(self.VMs[action]['Occuapted_memory'])/100\n",
        "    reward=reward/4\n",
        "    return next_state, reward,self.VMs\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    self.VMs=createVM(self.nVM,self.ProcessTime,self.QueueLength,self.ProcessorNumber)\n",
        "\n",
        "    state=[]\n",
        "    for i in range(0,self.nVM):\n",
        "      state.append(self.VMs[i]['L_Q'])\n",
        "\n",
        "    ####if normalized\n",
        "    # state=(lq0,lq1,lq2,lq3,lq4,lq5)\n",
        "    # state=np.reshape(state,(1,-1))\n",
        "    # state = preprocessing.normalize(state)[0]\n",
        "\n",
        "    return state,self.VMs\n",
        "  def print_output(self):\n",
        "      if self.print_mode==1:\n",
        "        for i in range(0,self.nVM):\n",
        "          print(\"-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\")\n",
        "          print(\"-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-Print Outputs-.-.-.-.-.-.-.-.-.-.-.-.\")\n",
        "          print(\"-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\")\n",
        "          print(\"Current Time is: \", str(self.time))\n",
        "          print( \"Occuapted_memory in VM \"+str(i)+\": \",self.VMs[i]['Occuapted_memory'])\n",
        "          print(\"InputTasks in VM \"+str(i)+\": \",self.VMs[i]['InputTasks'])\n",
        "          print(\"Finished Tasks in VM \"+str(i)+\": \",self.VMs[i]['FinishedTasks'])\n",
        "          print(\"Time of Finished Tasks in VM \"+str(i)+\": \",self.VMs[i]['Time_of_the_FinishedTasks'])\n",
        "          print( \"VM \"+str(i)+\": \",self.VMs[i])\n",
        "  def render(self):\n",
        "    frame_idx=self.frame_idx\n",
        "    scores=self.scores\n",
        "    losses: self.losses\n",
        "    epsilons: self.epsilons\n",
        "    \"\"\"Plot the training progresses.\"\"\"\n",
        "    clear_output(True)\n",
        "    plt.close()\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.subplot(121)\n",
        "    plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "    plt.plot(scores)\n",
        "    plt.subplot(122)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nVM=3\n",
        "ProcessorNumber=[8,8,12,12,16,16]\n",
        "ProcessTime=[30,30,20,20,10,10]\n",
        "QueueLength=[100,100,100,100,100,100]\n",
        "LB=[60,60,60,60,60,60]\n",
        "UB=QueueLength\n",
        "lower_bound=np.array(LB[0:nVM],dtype=np.float32,)\n",
        "upper_bound=np.array(UB[0:nVM],dtype=np.float32,)\n",
        "env=LoadBalancing(nVM,ProcessTime,QueueLength,ProcessorNumber,lower_bound,upper_bound)\n",
        "env.print_mode=0\n",
        "pm=env.print_mode"
      ],
      "metadata": {
        "id": "1Ij9bSHm51Ul"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EPISODES = 101\n",
        "max_steps=1000\n",
        "LEARNING_RATE = 1e-4\n",
        "LEARNING_RATE_DECAY = 0.99\n",
        "EXPLORATION_DECAY = 0.95\n",
        "GAMMA = 0.975\n",
        "UPDATE_TARGET_EVERY = 10\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "agent = DQN(\n",
        "    state_shape=env.observation_space.shape[0],\n",
        "    action_size=env.action_space.n,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate_max=LEARNING_RATE,\n",
        "    learning_rate_decay=LEARNING_RATE_DECAY,\n",
        "    exploration_decay=EXPLORATION_DECAY,\n",
        "    gamma=GAMMA\n",
        ")\n",
        "agent.save(f'models/-1.h5')\n",
        "\n",
        "state = env.reset()\n",
        "state = np.expand_dims(state, axis=0)\n",
        "\n",
        "most_recent_losses = deque(maxlen=BATCH_SIZE)\n",
        "\n",
        "log = []\n",
        "done=0\n",
        "\n",
        "# fill up memory before training starts\n",
        "while agent.memory.length() < BATCH_SIZE:\n",
        "    action = agent.act(state)\n",
        "    next_state, reward,VMs = env.step(action)\n",
        "\n",
        "    next_state = np.expand_dims(next_state, axis=0)\n",
        "    agent.remember(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "scores = []\n",
        "score = 0\n",
        "done=0\n",
        "inputtask=np.ndarray((EPISODES ,env.nVM))\n",
        "losses=[]\n",
        "for e in range(0, EPISODES):\n",
        "    state,VMs = env.reset()\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "    done = False\n",
        "    step = 0\n",
        "    all_input_tasks=0\n",
        "    env.VMs=VMs\n",
        "    env.start_time= time.time()\n",
        "    scores.append(np.mean(score))\n",
        "    ma_loss = None\n",
        "    score=0\n",
        "\n",
        "    for step in range(0,max_steps):\n",
        "\n",
        "      env.stp=step\n",
        "      env.time=1000*(time.time()-env.start_time)\n",
        "      env.time=np.int64(np.round(env.time))\n",
        "      action = agent.act(state)\n",
        "      next_state, reward,VMs = env.step(action)\n",
        "      env.VMs=VMs\n",
        "      next_state = np.expand_dims(next_state, axis=0)\n",
        "      agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "      score += reward\n",
        "\n",
        "      loss = agent.replay(episode=e)\n",
        "      most_recent_losses.append(loss)\n",
        "      ma_loss = np.array(most_recent_losses).mean()\n",
        "      losses.append(most_recent_losses)\n",
        "      env.print_output()\n",
        "      env.e=e\n",
        "    env.scores=scores\n",
        "    env.losses=losses\n",
        "\n",
        "    for i in range(0,env.nVM):\n",
        "      inputtask[e,i]=len(env.VMs[i]['InputTasks'])\n",
        "            # plotting\n",
        "    if e % 10 == 0:\n",
        "      env.render()\n",
        "      print_mode=1\n",
        "      env.print_output()\n",
        "      env.print_mode=0\n",
        "      env.print_mode=pm\n",
        "\n",
        "\n",
        "    log.append([e, step, score, agent.learning_rate, agent.exploration_rate, ma_loss])\n",
        "\n",
        "    agent.save(f'models/{e}.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "IGwPgv6m39nL",
        "outputId": "9e0c92f6-e8e3-4745-9d7c-55d4835f154b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c34f99cb73c1>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m agent = DQN(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mstate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-4b55200f565b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_shape, action_size, learning_rate_max, learning_rate_decay, gamma, memory_size, batch_size, exploration_max, exploration_min, exploration_decay)\u001b[0m\n\u001b[1;32m      3\u001b[0m                  memory_size=2000, batch_size=32, exploration_max=1.0, exploration_min=0.01, exploration_decay=0.995):\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"models\\5.h5\"\n",
        "agent = DQN(\n",
        "    state_shape=env.observation_space.shape[0],\n",
        "    action_size=env.action_space.n,\n",
        ")\n",
        "agent.load(model_path)\n",
        "\n",
        "state = env.reset()\n",
        "state = np.expand_dims(state, axis=0)\n",
        "\n",
        "import pygame\n",
        "pygame.init()\n",
        "screen = pygame.display.set_mode((env.WINDOW_WIDTH, env.WINDOW_HEIGHT))\n",
        "clock = pygame.time.Clock()\n",
        "running = True\n",
        "score = 0\n",
        "\n",
        "while running:\n",
        "    pygame.display.set_caption(f\"Score: {score}\")\n",
        "    for event in pygame.event.get():\n",
        "        if event.type == pygame.QUIT:\n",
        "            running = False\n",
        "\n",
        "    action = agent.act(state, 0)\n",
        "    state, reward, done, score = env.step(action)\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "\n",
        "    env.render(screen)\n",
        "    pygame.display.flip()\n",
        "    clock.tick(30)\n",
        "\n",
        "pygame.quit()"
      ],
      "metadata": {
        "id": "b4aZklhv4BVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "GC8Dqc11E4uA",
        "outputId": "ed40c098-c78b-4c10-a387-1600df29e80a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6997bd3c4372>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'state_shape' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}